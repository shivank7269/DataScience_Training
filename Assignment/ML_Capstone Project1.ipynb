{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Scenario Question: Predicting Titanic Survival\n",
    "Researchers are studying the Titanic disaster and want to build models that predict whether a\n",
    " passenger would survive or not survive based on their information.\n",
    "- Features used:\n",
    "- Passenger class (pclass)\n",
    "- Gender (sex)\n",
    "- Age (age)\n",
    "- Number of siblings/spouses aboard (sibsp)\n",
    "- Number of parents/children aboard (parch)\n",
    "- Ticket fare (fare)\n",
    "- Label:\n",
    "- 1 = Survived\n",
    "- 0 = Died\n",
    "The researchers train three different models:\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN) with k=5\n",
    "- Decision Tree with max depth = 4\n",
    "They then evaluate each model using a classification report (precision, recall, F1-score, accuracy).\n",
    "\n",
    "❓ Questions for Learners\n",
    "- Which model performs best at predicting survival, and why?\n",
    "- How does Logistic Regression differ from Decision Tree in terms of interpretability?\n",
    "# - Why is scaling applied before training Logistic Regression and KNN, but not strictly needed\n",
    " for Decision Trees?\n",
    "- Looking at the classification report, what do precision and recall mean in the context of survival\n",
    " predictions?\n",
    "- Precision → Of those predicted to survive, how many actually survived?\n",
    "- Recall → Of all who truly survived, how many were correctly predicted?\n",
    "- If you were a historian, which model would you trust more to explain survival patterns, and why?"
   ],
   "id": "784c59cfebd484dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-28T09:56:50.571894400Z",
     "start_time": "2026-02-28T09:56:50.458545200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression      # for logistic classification\n",
    "from sklearn.neighbors import KNeighborsClassifier       # for KNeighbor classification\n",
    "from sklearn.tree import DecisionTreeClassifier          # for decision tree classification\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score,recall_score,accuracy_score\n",
    "\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "df['age'] = df['age'].fillna(df['age'].median())\n",
    "df['sex'] = df['sex'].map({'male':0,'female':1})\n",
    "X=df[['pclass','sex','age','sibsp','parch','fare']]\n",
    "y=df['survived']\n",
    "\n",
    "# SPLIT the data into 80% training and 20% test\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X,y,test_size=0.2,random_state=42\n",
    ")\n",
    "\n",
    "f1_scores = {}\n",
    "accuracy_scores = {}\n",
    "recall_scores = {}\n",
    "precision_scores = {}\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION REPORT\\n\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_scores['Logistic'] = f1_score(y_test,y_pred)\n",
    "accuracy_scores['Logistic'] = accuracy_score(y_test,y_pred)\n",
    "precision_scores['Logistic'] = precision_score(y_test,y_pred)\n",
    "recall_scores['Logistic'] = recall_score(y_test,y_pred)\n",
    "\n",
    "# KNeighbor Classification\n",
    "\n",
    "k = KNeighborsClassifier(n_neighbors=5)\n",
    "k.fit(X_train,y_train)\n",
    "\n",
    "y_pred = k.predict(X_test)\n",
    "\n",
    "print(\"KNN REPORT\\n\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_scores['KNN'] = f1_score(y_test,y_pred)\n",
    "accuracy_scores['KNN'] = accuracy_score(y_test,y_pred)\n",
    "precision_scores['KNN'] = precision_score(y_test,y_pred)\n",
    "recall_scores['KNN'] = recall_score(y_test,y_pred)\n",
    "\n",
    "# Decision Tree\n",
    "\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=4,criterion='gini',random_state=42   # to make split always same\n",
    ")\n",
    "tree.fit(X_train,y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "print(\"DECISION TREE REPORT\\n\")\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_scores['Decision Tree'] = f1_score(y_test,y_pred)\n",
    "accuracy_scores['Decision Tree'] = accuracy_score(y_test,y_pred)\n",
    "precision_scores['Decision Tree'] = precision_score(y_test,y_pred)\n",
    "recall_scores['Decision Tree'] = recall_score(y_test,y_pred)\n",
    "\n",
    "# COMPARISON\n",
    "\n",
    "best_model_f1_score = max(f1_scores, key=f1_scores.get)\n",
    "best_model_accuracy_score = max(accuracy_scores, key=accuracy_scores.get)\n",
    "best_model_precision_score = max(precision_scores, key=precision_scores.get)\n",
    "best_model_recall_score = max(recall_scores, key=recall_scores.get)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(\"Best Model based on F1 Score:\", best_model_f1_score)\n",
    "print(\"Best Model based on Accuracy Score:\", best_model_accuracy_score)\n",
    "print(\"Best Model based on Precision Score:\", best_model_precision_score)\n",
    "print(\"Best Model based on Recall Score:\", best_model_recall_score)\n",
    "\n"
   ],
   "id": "c879d4c6be8d1edd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION REPORT\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       105\n",
      "           1       0.80      0.72      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "KNN REPORT\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77       105\n",
      "           1       0.68      0.53      0.60        74\n",
      "\n",
      "    accuracy                           0.70       179\n",
      "   macro avg       0.70      0.68      0.68       179\n",
      "weighted avg       0.70      0.70      0.70       179\n",
      "\n",
      "DECISION TREE REPORT\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       105\n",
      "           1       0.80      0.69      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       105\n",
      "           1       0.80      0.69      0.74        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Best Model based on F1 Score: Logistic\n",
      "Best Model based on Accuracy Score: Logistic\n",
      "Best Model based on Precision Score: Logistic\n",
      "Best Model based on Recall Score: Logistic\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
